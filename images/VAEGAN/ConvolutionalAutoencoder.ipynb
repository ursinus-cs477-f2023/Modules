{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a76b1d9",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a24d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61939422",
   "metadata": {},
   "source": [
    "## Dataset Definitions / Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "#Converting data to torch.FloatTensor and padding to 32x32\n",
    "transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n",
    "data_train = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "# Use both datasets to maximize info\n",
    "data = torch.utils.data.ConcatDataset([data_train, data_test]) \n",
    "\n",
    "DIGIT_RES = data_train[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_train[0]\n",
    "# num channels x height x width\n",
    "print(X.shape)\n",
    "plt.imshow(X[0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "X1 = layer1(X)\n",
    "print(\"X1.shape\", X1.shape)\n",
    "layer2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "X2 = layer2(X1)\n",
    "print(\"X2.shape\", X2.shape)\n",
    "for i, weights in enumerate(layer2.parameters()):\n",
    "    print(\"layer 2 weight set {}\".format(i), weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79936210",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99064b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, digit_res, depth=4, dim_latent=2, dim_img=32, in_channels=1):\n",
    "        \"\"\"\n",
    "        digit_res: int\n",
    "            Resolution of digit\n",
    "        depth: int\n",
    "            How many convolutional layers there are in the encoder/decoder\n",
    "        dim_latent: int\n",
    "            Dimension of the latent space\n",
    "        dim_digit: int\n",
    "            Width/height of input image\n",
    "        in_channels: int\n",
    "            Number of channels of input image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_latent = dim_latent\n",
    "        \n",
    "        ## Step 1: Create convolutional encoder\n",
    "        in_orig = in_channels\n",
    "        layers = []\n",
    "        out_channels = 16\n",
    "        for i in range(depth):\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        # Create a dummy input to get the shape right\n",
    "        X = torch.zeros(1, in_orig, dim_img, dim_img)\n",
    "        XOut = nn.Sequential(*layers)(X)\n",
    "        shape = XOut.shape[1::]\n",
    "        layers += [nn.Flatten(), nn.Linear(np.prod(shape), dim_latent), nn.Sigmoid()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        ## Step 2: Setup convolutional decoder\n",
    "        layers = [nn.Linear(dim_latent, np.prod(shape)), nn.LeakyReLU(), nn.Unflatten(1, shape)]\n",
    "        in_channels = out_channels//2\n",
    "        for i in range(depth):\n",
    "            out_channels = 1\n",
    "            if i < depth-2:\n",
    "                out_channels = in_channels // 2\n",
    "            # Use upsampling with bilinear interpolation instead of ConvTranspose\n",
    "            # to avoid checkerboard artifacts\n",
    "            # See this link for more info: https://distill.pub/2016/deconv-checkerboard/\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='bilinear'))\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1))\n",
    "            layers.append(nn.LeakyReLU()) # I forgot this in the video!\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = self.encoder(X) # Encoding in latent space\n",
    "        XOut = self.decoder(z) # Decoding\n",
    "        loss = torch.sum((X-XOut)**2)\n",
    "        return z, XOut, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c453596",
   "metadata": {},
   "source": [
    "## Plotting Code for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a26b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_digits(model, data, device, n_scatter=1000):\n",
    "    \"\"\"\n",
    "    Scatter a subset of digits in their latent representation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Autoencoder model\n",
    "    data: torch dataset\n",
    "        Digits dataset\n",
    "    device: str\n",
    "        Device on which to run the model\n",
    "    n_scatter: int\n",
    "        Number of example digits to scatter\n",
    "    \"\"\"\n",
    "    from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "    ax = plt.gca()\n",
    "    encoded = []\n",
    "    # Convert a grayscale digit to one with a background color chosen from\n",
    "    # the tab10 colorcycle to indicate its class\n",
    "    c = plt.get_cmap(\"tab10\")\n",
    "    jump = len(data)//n_scatter\n",
    "    for k in range(n_scatter):\n",
    "        tidx = k*jump\n",
    "        label = data[tidx][1]\n",
    "        img = data[tidx][0].to(device)\n",
    "        z, _, _ = model(img.unsqueeze(0))\n",
    "        img = img.detach().cpu()[0, :, :].numpy()\n",
    "        x, y = z[0, :].detach().cpu()\n",
    "        encoded.append([x, y])\n",
    "        C = c([label]).flatten()[0:3]\n",
    "        img_disp = np.zeros((img.shape[0], img.shape[1], 4))\n",
    "        img_disp[:, :, 0:3] = img[:, :, None]*C[None, None, :]\n",
    "        img_disp[:, :, 3] = img\n",
    "        img_disp = OffsetImage(img_disp, zoom=0.7)\n",
    "        ab = AnnotationBbox(img_disp, (x, y), xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "    encoded = np.array(encoded)\n",
    "    ax.update_datalim(encoded)\n",
    "    ax.autoscale()\n",
    "\n",
    "def plot_digits_dimreduced_examples(model, data, device, n_examples=20):\n",
    "    \"\"\"\n",
    "    Plot examples of encoded digits, as well as a scatter of some digits\n",
    "    in their latent representation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Autoencoder model\n",
    "    data: torch dataset\n",
    "        Digits dataset\n",
    "    device: str\n",
    "        Device on which to run the model\n",
    "    n_examples: int\n",
    "        Number of example encodings to show\n",
    "    \"\"\"\n",
    "    ## Step 1: Plot examples of encodings\n",
    "    jump = len(data)//n_examples\n",
    "    for k in range(n_examples):\n",
    "        tidx = k*jump\n",
    "        x = data[tidx][0].to(device)\n",
    "        z, xenc, _ = model(x.unsqueeze(0))\n",
    "        x = x.detach().cpu()[0, :, :]\n",
    "        xenc = xenc.detach().cpu()[0, 0, :, :]\n",
    "        \n",
    "        plt.subplot(n_examples, n_examples, k+1)\n",
    "        plt.imshow(x, vmin=0, vmax=1, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(n_examples, n_examples, n_examples+k+1)\n",
    "        plt.imshow(xenc, vmin=0, vmax=1, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    ## Step 2: Do a scatterplot of a subset of the digits in their latent space\n",
    "    plt.subplot2grid((n_examples, n_examples), (2, 0), colspan=n_examples, rowspan=n_examples-2)\n",
    "    scatter_digits(model, data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = ConvAutoencoder(32)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) ## I made the learning rate smaller from the video\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 16\n",
    "train_losses = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    train_loss = 0\n",
    "    for i, (X, Y) in enumerate(loader): # Go through each mini batch\n",
    "        X = X.to(device)\n",
    "        # Reset the optimizer's gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Run the sequential model on all inputs\n",
    "        _, _, loss = model(X)\n",
    "        # Compute the gradients of the loss function with respect\n",
    "        # to all of the parameters of the model\n",
    "        loss.backward()\n",
    "        # Update the parameters based on the gradient and\n",
    "        # the optimization scheme\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            ipd.clear_output()\n",
    "            print(\"Epoch {} batch {}: loss {:.3f}\".format(epoch, i, train_loss/((i+1)*batch_size)))\n",
    "    plt.clf()\n",
    "    model.eval()\n",
    "    plot_digits_dimreduced_examples(model, data_train, device)\n",
    "    plt.savefig(\"Epoch{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "for p in layer.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d97223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
