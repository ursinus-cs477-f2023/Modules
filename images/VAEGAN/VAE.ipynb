{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2473b574",
   "metadata": {},
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699eef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f387e",
   "metadata": {},
   "source": [
    "## Dataset Definitions / Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "#Converting data to torch.FloatTensor and padding to 32x32\n",
    "transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n",
    "data_train = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "data_test = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "# Use both datasets to maximize info\n",
    "data = torch.utils.data.ConcatDataset([data_train, data_test]) \n",
    "\n",
    "DIGIT_RES = data_train[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244fa8c",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, digit_res, depth=4, dim_latent=2, dim_img=32, in_channels=1):\n",
    "        \"\"\"\n",
    "        digit_res: int\n",
    "            Resolution of digit\n",
    "        depth: int\n",
    "            How many convolutional layers there are in the encoder/decoder\n",
    "        dim_latent: int\n",
    "            Dimension of the latent space\n",
    "        dim_digit: int\n",
    "            Width/height of input image\n",
    "        in_channels: int\n",
    "            Number of channels of input image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_latent = dim_latent\n",
    "        \n",
    "        ## Step 1: Create convolutional encoder\n",
    "        in_orig = in_channels\n",
    "        layers = []\n",
    "        out_channels = 16\n",
    "        for i in range(depth):\n",
    "            layers.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        # Create a dummy input to get the shape right\n",
    "        X = torch.zeros(1, in_orig, dim_img, dim_img)\n",
    "        XOut = nn.Sequential(*layers)(X)\n",
    "        shape = XOut.shape[1::]\n",
    "        layers += [nn.Flatten()]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Create a linear layer for both mu and sigma\n",
    "        self.mu_enc = nn.Sequential(nn.Linear(np.prod(shape), dim_latent), nn.Tanh())\n",
    "        self.sigma_enc = nn.Sequential(nn.Linear(np.prod(shape), dim_latent), nn.ReLU())\n",
    "        \n",
    "        ## Step 2: Setup convolutional decoder\n",
    "        layers = [nn.Linear(dim_latent, np.prod(shape)), nn.LeakyReLU(), nn.Unflatten(1, shape)]\n",
    "        in_channels = out_channels//2\n",
    "        for i in range(depth):\n",
    "            out_channels = 1\n",
    "            if i < depth-2:\n",
    "                out_channels = in_channels // 2\n",
    "            # Use upsampling with bilinear interpolation instead of ConvTranspose\n",
    "            # to avoid checkerboard artifacts\n",
    "            # See this link for more info: https://distill.pub/2016/deconv-checkerboard/\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='bilinear'))\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        flat_dim = self.encoder(X)\n",
    "        # Estimate mu and sigma\n",
    "        mu = 5*self.mu_enc(flat_dim)\n",
    "        sigma = self.sigma_enc(flat_dim)\n",
    "        # Use reparam trick to sample a z in the latent space\n",
    "        z = mu + sigma*torch.randn(sigma.shape).to(sigma)\n",
    "        XOut = self.decoder(z) # Decoding\n",
    "        \n",
    "        # Fit the data\n",
    "        loss_fit = torch.sum((X-XOut)**2)\n",
    "        # Fit the prior (mu=0, sigma=1)\n",
    "        kl_div = 0.5*torch.sum(sigma**2 + mu**2 - 1 - torch.log(1e-8 + sigma**2), dim=1)\n",
    "        kl_div = torch.mean(kl_div)\n",
    "        \n",
    "        return z, XOut, (loss_fit, kl_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca813962",
   "metadata": {},
   "source": [
    "## Plotting Code for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_digits(model, data, device, n_scatter=1000):\n",
    "    \"\"\"\n",
    "    Scatter a subset of digits in their latent representation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Autoencoder model\n",
    "    data: torch dataset\n",
    "        Digits dataset\n",
    "    device: str\n",
    "        Device on which to run the model\n",
    "    n_scatter: int\n",
    "        Number of example digits to scatter\n",
    "    \"\"\"\n",
    "    from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "    ax = plt.gca()\n",
    "    encoded = []\n",
    "    # Convert a grayscale digit to one with a background color chosen from\n",
    "    # the tab10 colorcycle to indicate its class\n",
    "    c = plt.get_cmap(\"tab10\")\n",
    "    jump = len(data)//n_scatter\n",
    "    for k in range(n_scatter):\n",
    "        tidx = k*jump\n",
    "        label = data[tidx][1]\n",
    "        img = data[tidx][0].to(device)\n",
    "        z, _, _ = model(img.unsqueeze(0))\n",
    "        img = img.detach().cpu()[0, :, :].numpy()\n",
    "        x, y = z[0, :].detach().cpu()\n",
    "        encoded.append([x, y])\n",
    "        C = c([label]).flatten()[0:3]\n",
    "        img_disp = np.zeros((img.shape[0], img.shape[1], 4))\n",
    "        img_disp[:, :, 0:3] = img[:, :, None]*C[None, None, :]\n",
    "        img_disp[:, :, 3] = img\n",
    "        img_disp = OffsetImage(img_disp, zoom=0.7)\n",
    "        ab = AnnotationBbox(img_disp, (x, y), xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "    encoded = np.array(encoded)\n",
    "    ax.update_datalim(encoded)\n",
    "    ax.autoscale()\n",
    "    return ax\n",
    "\n",
    "def plot_digits_dimreduced_examples(model, data, device, n_examples=20):\n",
    "    \"\"\"\n",
    "    Plot examples of encoded digits, as well as a scatter of some digits\n",
    "    in their latent representation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Autoencoder model\n",
    "    data: torch dataset\n",
    "        Digits dataset\n",
    "    device: str\n",
    "        Device on which to run the model\n",
    "    n_examples: int\n",
    "        Number of example encodings to show\n",
    "    \"\"\"\n",
    "    ## Step 1: Plot examples of encodings\n",
    "    jump = len(data)//n_examples\n",
    "    for k in range(n_examples):\n",
    "        tidx = k*jump\n",
    "        x = data[tidx][0].to(device)\n",
    "        z, xenc, _ = model(x.unsqueeze(0))\n",
    "        x = x.detach().cpu()[0, :, :]\n",
    "        xenc = xenc.detach().cpu()[0, 0, :, :]\n",
    "        \n",
    "        plt.subplot(n_examples, n_examples, k+1)\n",
    "        plt.imshow(x, vmin=0, vmax=1, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(n_examples, n_examples, n_examples+k+1)\n",
    "        plt.imshow(xenc, vmin=0, vmax=1, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    ## Step 2: Do a scatterplot of a subset of the digits in their latent space\n",
    "    plt.subplot2grid((n_examples, n_examples), (2, 0), colspan=n_examples, rowspan=n_examples-2)\n",
    "    return scatter_digits(model, data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e9bfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = ConvAutoencoder(32)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 16\n",
    "train_losses = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "lam = 30\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    train_loss_fit = 0\n",
    "    train_loss_prior = 0\n",
    "    for i, (X, Y) in enumerate(loader): # Go through each mini batch\n",
    "        X = X.to(device)\n",
    "        # Reset the optimizer's gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Run the sequential model on all inputs\n",
    "        _, _, (loss_fit, loss_prior) = model(X)\n",
    "        # Compute the gradients of the loss function with respect\n",
    "        # to all of the parameters of the model\n",
    "        loss = loss_fit + lam*loss_prior\n",
    "        loss.backward()\n",
    "        # Update the parameters based on the gradient and\n",
    "        # the optimization scheme\n",
    "        optimizer.step()\n",
    "        train_loss_fit += loss_fit.item()\n",
    "        train_loss_prior += loss_prior.item()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            ipd.clear_output()\n",
    "            print(\"Epoch {} batch {}: loss fit {:.3f}, loss prior {:.3f}\".format(\n",
    "                epoch, i, train_loss_fit/((i+1)*batch_size),\n",
    "                        train_loss_prior/((i+1)*batch_size)))\n",
    "    plt.clf()\n",
    "    model.eval()\n",
    "    ax = plot_digits_dimreduced_examples(model, data, device)\n",
    "    ax.text(ax.get_xlim()[0]+0.1, ax.get_ylim()[1]-0.2, \"$\\\\lambda={}$, loss fit: {:.3f}, loss prior: {:.3f}\".format(\n",
    "    lam, train_loss_fit/len(data), train_loss_prior/len(data)))\n",
    "    plt.savefig(\"Epoch{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a33062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
