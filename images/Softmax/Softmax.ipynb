{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fb87b8",
   "metadata": {},
   "source": [
    "# Softmax (Multi-Class Classification)\n",
    "\n",
    "Multiple possible outputs\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td><h3>Activation Function</h3></td>\n",
    "        <td><h3>Loss Function</h3></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Logistic Regression<p>(Binary Classification)</p></td>\n",
    "        <td>Logistic Function: <h3>$\\frac{1}{1+e^{-u}}$</h3></td>\n",
    "        <td>Logistic loss</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Multi-Class Logistic Regression <p>(Learn one of many options)</p></td>\n",
    "        <td>Vector <b>u</b> as input\n",
    "            $\\sigma_{sm}(\\vec{u})_i = \\frac{e^{u_i}}{\\sum_{j = 1}^{L} e^{u_j}}$</td>\n",
    "        <td>\n",
    "            Binary Cross-Entropy (BCE)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e38cea",
   "metadata": {},
   "source": [
    "Softmax emphasizes relatively larger values\n",
    "\n",
    "* Each element of the softmax is nonnegative\n",
    "* The sum of of all of the elements is 1 \n",
    "\n",
    "Interpret the input the softmax as log probabilities\n",
    "Output of softmax as estimated probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc503d",
   "metadata": {},
   "source": [
    "<img src = \"softmaxexamples.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9ca15",
   "metadata": {},
   "source": [
    "For multi-class classification, we're going to have a multidimensional \"ground truth\" $y$ that we aspire to.  So I'll define something called the \"one-hot vector\" $\\vec{y}^k$ as\n",
    "\n",
    "### $\\vec{y}^k_i = \\left\\{ \\begin{array}{cc} 1 & i = k \\\\ 0 & \\text{otherwise} \\end{array} \\right\\} $\n",
    "\n",
    "(this is also referred to as the \"standard basis vector\" $\\vec{e}_i$ in $k$-dimensional Euclidean space)\n",
    "\n",
    "Ex) $\\vec{y}^6 = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c836e",
   "metadata": {},
   "source": [
    "Binary cross-entropy loss between an input $\\vec{u}$ to the softmax and a 1-hot vector $y_i^{k}$ is defined as\n",
    "\n",
    "### $BCE(\\vec{u}, \\vec{y^k}) = -\\sum_{i} \\vec{y}^k_i \\log( \\sigma_{sm}(\\vec{u})_i )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9a8e7",
   "metadata": {},
   "source": [
    "BCE is 0 if and if only if the outputs of the softmax exactly match the 1-hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948f9d7",
   "metadata": {},
   "source": [
    "Def. Entropy of a probability mass function (PMF)\n",
    "NOTE: A PMF is an array of possible outcomes with the probabilities\n",
    "\n",
    "Ex) PMF $X = [0:0.2, 1:0.5, 2:0.3]$\n",
    "\n",
    "$H(X) = \\sum_{i} p_i \\log_2(1/p_i)$\n",
    "\n",
    "$H(X) = -\\sum_{i} p_i \\log_2(p_i)$\n",
    "\n",
    "Entropy is a measure of the expected number of bits that I should use in an optimal representation of my random variable.  Have a look back at <a href = \"https://ursinus-cs174-s2023.github.io/CoursePage/Labs/Lab7_HuffmanTrees/\">Huffman trees from CS 174</a> for an example of using more bits for less likely things and vice versa.\n",
    "\n",
    "Cross-entropy is a measure of the expected number of bits that I have to use if I assume the wrong distribution (in this case the output of the softmax) when using probabilities from the target distribution (in this case the 1-hot vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d920b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log2(1/0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6e7599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273344"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2*np.log2(1/0.2) + 0.5*np.log2(1/0.5) + 0.3*np.log2(1/0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adc9a7",
   "metadata": {},
   "source": [
    "# Update Rules\n",
    "\n",
    "We want the derivatites of the BCE loss with respect to different components $u_i$ of the input $\\vec{u}$\n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_i} \\left( -\\sum_{i} \\vec{y}^k_i \\log( \\sigma_{sm}(\\vec{u})_i ) \\right)$\n",
    "\n",
    "There's only one component that actually matters when we're using the one-hot vector $\\vec{y}^k_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b497a2a",
   "metadata": {},
   "source": [
    "## $\\frac{\\partial}{\\partial u_i} \\left( - \\log( \\sigma_{sm}(\\vec{u})_k ) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c938bb",
   "metadata": {},
   "source": [
    "\n",
    "### Case 1:\n",
    "Let's suppose that $i = k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006770ed",
   "metadata": {},
   "source": [
    "By the chain rule, we have\n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_k} \\left( - \\log( \\sigma_{sm}(\\vec{u})_k ) \\right) =  -\\frac{1}{\\sigma_{sm}(\\vec{u})_k }  \\left( \\frac{\\partial}{\\partial u_k} \\sigma_{sm}(\\vec{u})_k \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6501a",
   "metadata": {},
   "source": [
    "\n",
    "Let's look at the derivative of the inner:\n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_k} \\sigma_{sm}(\\vec{u})_k  = \\frac{\\partial}{\\partial u_k} \\frac{e^{u_k}}{\\sum_{j = 1}^{L} e^{u_j}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40b605",
   "metadata": {},
   "source": [
    "Quotient Rule: \n",
    "$f(x) = P(x)/Q(x),  f'(x) = (P'(x)Q(x) - Q'(x)P(x))/(Q(x)^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e68a6",
   "metadata": {},
   "source": [
    "$P(u_k) = e^{u_k}$\n",
    "\n",
    "$Q(u_k) = \\sum_{j = 1}^{L} e^{u_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc410d1",
   "metadata": {},
   "source": [
    "$P'(u_k) = Q'(u_k) =  e^{u_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ad286",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_k} \\frac{e^{u_k}}{\\sum_{j = 1}^{L} e^{u_j}} = \\frac{e^{u_k}(\\sum_{j = 1}^{L} e^{u_j}) - e^{u_k} e^{u_k}}{(\\sum_{j = 1}^{L} e^{u_j})^2} = \\left( \\frac{e^{u_k}}{\\sum_{j = 1}^{L} e^{u_j}} \\right) \\left( \\frac{(\\sum_{j = 1}^{L} e^{u_j}) - e^{u_k}}{\\sum_{j = 1}^{L} e^{u_j}} \\right) = \\sigma_{sm}(\\vec{u})_k (1 - \\sigma_{sm}(\\vec{u})_k) $\n",
    "\n",
    "Plugging this back into the full derivative of BCE, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfcc83b",
   "metadata": {},
   "source": [
    "\n",
    "## $ -\\frac{1}{\\sigma_{sm}(\\vec{u})_k }  \\sigma_{sm}(\\vec{u})_k (1 - \\sigma_{sm}(\\vec{u})_k) $\n",
    "\n",
    "which simplifies to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366bbcc",
   "metadata": {},
   "source": [
    "## $-(1 - \\sigma_{sm}(\\vec{u})_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5189bb",
   "metadata": {},
   "source": [
    "For the hot $k$, the update rule is\n",
    "\n",
    "### $u_k \\gets u_k + \\alpha(1 - \\sigma_{sm}(\\vec{u})_k)$\n",
    "\n",
    "In other words, if we're not big enough at our softmax output, we should nudge $u_k$ to the right.  This will make $\\frac{e^{u_k}}{\\sum_{j = 1}^{L} e^{u_j}}$ closer to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55aff8b",
   "metadata": {},
   "source": [
    "### Case 2:\n",
    "Let's suppose that $i \\neq k$\n",
    "\n",
    "This is a \"cold zero\"\n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_i} \\left( - \\log( \\sigma_{sm}(\\vec{u})_k ) \\right) = - \\frac{1}{\\sigma_{sm}(\\vec{u})_k} \\frac{\\partial}{\\partial u_i} \\sigma_{sm}(\\vec{u})_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572c08c",
   "metadata": {},
   "source": [
    "Let's take the derivative of the inner using the quotient rule again\n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_i} \\sigma_{sm}(\\vec{u})_k = \\frac{\\partial}{\\partial u_i} \\frac{e^{u_k}}{\\sum_{j = 1}^{L} e^{u_j}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6fb68",
   "metadata": {},
   "source": [
    "$P(u_i) = e^{u_k}$\n",
    "\n",
    "$Q(u_i) = \\sum_{j=1}^L e^{u_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d991b4",
   "metadata": {},
   "source": [
    "Because we're taking the derivative with respect to $u_i$, and $i \\neq k$, we treat $u_k$ as a constant.  Therefore,\n",
    "\n",
    "$P'(u_i) = 0$\n",
    "\n",
    "$Q'(u_i) = e^{u_i}$\n",
    "\n",
    "and we end up with\n",
    "\n",
    "## $\\frac{\\partial}{\\partial u_i} \\sigma_{sm}(\\vec{u})_k = \\frac{-e^{u_i} e^{u_k}}{(\\sum_{j=1}^L e^{u_j})^2} = -\\sigma_{sm}(\\vec{u})_i \\sigma_{sm}(\\vec{u})_k   $\n",
    "\n",
    "\n",
    "Plugging this back into the derivative of BCE loss, we have the following incredibly simple expression after a cancellation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b128",
   "metadata": {},
   "source": [
    "## $ - \\frac{1}{\\sigma_{sm}(\\vec{u})_k} (-\\sigma_{sm}(\\vec{u})_i \\sigma_{sm}(\\vec{u})_k) = \\sigma_{sm}(\\vec{u})_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d2139",
   "metadata": {},
   "source": [
    "The update rule for gradient descent for a cold 0 is then\n",
    "\n",
    "### $u_i \\gets u_i - \\alpha \\sigma_{sm}(\\vec{u})_i$\n",
    "\n",
    "In other words, if our softmax output is greater than 0 at our cold 0, we should nudge $u_i$ to the left.  This will make $\\frac{e^{u_i}}{\\sum_{j = 1}^{L} e^{u_j}}$ closer to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d125403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
